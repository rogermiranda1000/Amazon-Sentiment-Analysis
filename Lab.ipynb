{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b179ce6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import re\n",
    "import json\n",
    "import datetime\n",
    "from gensim.models import word2vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import multiprocessing # number of threads\n",
    "\n",
    "from typing import Dict,Any,List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c523f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysis():\n",
    "    def __init__(self, json_file, hidden_layer = (300,50), random_state: int = None):\n",
    "        self._reviews=json.load(gzip.open(json_file,'rb'))\n",
    "        self._hidden_layer = hidden_layer\n",
    "        self._random_state = random_state\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def reviews(self) -> List[Dict[str,Any]]:\n",
    "        return self._reviews\n",
    "    \n",
    "    @property\n",
    "    def largest_review_size(self) -> int:\n",
    "        # @ref https://stackoverflow.com/a/1582670/9178470\n",
    "        return len(max(self.review_sentences, key=len))\n",
    "        \n",
    "    @property\n",
    "    def review_sentences(self) -> List[str]:\n",
    "        return [x['text'] for x in self._reviews]\n",
    "    \n",
    "    def makeBoW(self):\n",
    "        count_vectorizer=CountVectorizer()\n",
    "        counts=count_vectorizer.fit_transform([\" \".join(x['text']) for x in self._reviews])\n",
    "        transformer = TfidfTransformer()\n",
    "        self.BoW=transformer.fit_transform(counts)\n",
    "        #Posem els scores en un array\n",
    "        self.scores=np.array([x['score'] for x in self._reviews])\n",
    "\n",
    "    def divideTrainTest(self):\n",
    "        is2013=[x['year']==2013 for x in self._reviews]\n",
    "\n",
    "        self.BoW_train=self.BoW[~np.array(is2013),:]\n",
    "        self.scores_train=self.scores[~np.array(is2013)]\n",
    "        self.BoW_test=self.BoW[np.array(is2013),:]\n",
    "        self.scores_test=self.scores[np.array(is2013)]\n",
    "        pass\n",
    "\n",
    "    def saveTrainTest(self):\n",
    "        #Mireu np.save i (np.load Opcional)\n",
    "        pass\n",
    "    def learn(self):\n",
    "        #To be fit by the student\n",
    "        learner=MLPClassifier(hidden_layer_sizes=self._hidden_layer, solver='lbfgs', alpha=1e-5, random_state=self._random_state)\n",
    "        self.model=learner.fit(self.BoW_train,self.scores_train)\n",
    "        pass\n",
    "    \n",
    "    def evaluate(self):\n",
    "        #Returns RMSE\n",
    "        print(np.sqrt(sum(np.power(self.scores_test-self.model.predict(self.BoW_test),2))))\n",
    "        print(confusion_matrix(self.scores_test,np.round(self.model.predict(self.BoW_test))))\n",
    "        print(classification_report(self.scores_test,np.round(self.model.predict(self.BoW_test))))\n",
    "\n",
    "    def train_word2vec_model(self, size: int, window: int = 5, min_count: int = 5, workers: int = -1) -> word2vec.Word2Vec:\n",
    "        \"\"\"\n",
    "        Get a word2vec model according to the input data.\n",
    "        @ref https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
    "        \n",
    "        @param size:      Dimensionality of the word vectors\n",
    "        @param window:    Maximum distance between the current and predicted word\n",
    "        @param min_count: Ignores all words with total frequency lower than this\n",
    "        @param workers:   Use these many worker threads to train the model; -1 to use all\n",
    "        @return Word2Vec model\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        sentences=self.review_sentences\n",
    "        \n",
    "        if workers == -1:\n",
    "            workers = SentimentAnalysis.get_num_max_workers()\n",
    "            print(f\"[v] Using {workers} workers for the Word2Vec operation...\")\n",
    "            \n",
    "        return word2vec.Word2Vec(sentences, vector_size=size, window=window,\n",
    "                                       min_count=min_count, workers=workers)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_num_max_workers() -> int:\n",
    "        return multiprocessing.cpu_count()\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_word2vec(model: word2vec.Word2Vec, file_name: str):\n",
    "        model.save(file_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_word2vec(file_name: str) -> word2vec.Word2Vec:\n",
    "        return word2vec.Word2Vec.load(file_name)\n",
    "\n",
    "    def clusterize(self, model: word2vec.Word2Vec):\n",
    "        # Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "        # average of 30 words per cluster\n",
    "\n",
    "        word_vectors = model.wv.vectors\n",
    "        num_clusters = 10\n",
    "\n",
    "        # Initalize a k-means object and use it to extract centroids\n",
    "        kmeans_clustering = KMeans( n_clusters = num_clusters, n_init='auto' )\n",
    "        idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "        # Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "        # a cluster number\n",
    "        word_centroid_map = dict(list(zip( model.wv.index_to_key, idx )))\n",
    "\n",
    "        # For the first 10 clusters\n",
    "        for cluster in range(0,10):\n",
    "            #\n",
    "            # Print the cluster number\n",
    "            print(\"\\nCluster %d\" % cluster)\n",
    "            #\n",
    "            # Find all of the words for that cluster number, and print them out\n",
    "            words = []\n",
    "            for i in range(0,len(list(word_centroid_map.values()))):\n",
    "                if( list(word_centroid_map.values())[i] == cluster ):\n",
    "                    words.append(list(word_centroid_map.keys())[i])\n",
    "            print(words[:10])\n",
    "\n",
    "        pass\n",
    "\n",
    "# global variables\n",
    "WORD2VEC_VECTOR_SIZE = 300\n",
    "\n",
    "# create an inscance of the class\n",
    "SaHandler=SentimentAnalysis('data/Watches_withstopwords.json.gz', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18eca26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0: having owned two previous g shocks in my life including the first series in 1984 i ve long appreciated their quality the main reason i stopped wearing them was simply because the resin straps would break and having worn them for 4 years i grew weary of them however having burned through many fashion watches in the last 10 years i ve been disappointed to spend only to get 1 or 2 years worth of usage so i m back to g shock i think this model g1710d 7av represents a nice blend between good looks and practical durability which most guys really want the face is smaller on my wrist than i expected from a g shock but i think it s still classy for the office the side buttons are hidden in gray plastic which you can t see in picture but the rest of the watch is metal you might find a classier analog face g shock in the 200 300 but this one is a better value the only reason i didn t give it 5 stars is because the led isn t backlight it s an amber light that comes around the inside bevel so the led helps you read the dial hands but not the lcd screens nonetheless it s still a cool light color ps the bottom lcd screen isn t blue as pictured but it s still a different color than the top lcd which adds further distinction to the watch\n"
     ]
    }
   ],
   "source": [
    "print(f\"{SaHandler.reviews[0]['score']}: {' '.join(SaHandler.reviews[0]['text'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb0cc499",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4187\n"
     ]
    }
   ],
   "source": [
    "max_input_size = SaHandler.largest_review_size\n",
    "print(max_input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44819367",
   "metadata": {},
   "source": [
    "Here we'll train a Word2Vec model using the train data.\n",
    "Word2Vec will learn word associations from a large corpus of text. It will be able to detect synonymous words,[ref](https://en.wikipedia.org/w/index.php?title=Word2vec&oldid=1143734439) as words are represented by vectors that states the context of that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "674e3b7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v] Using 32 workers for the Word2Vec operation...\n"
     ]
    }
   ],
   "source": [
    "word2vec_model = SaHandler.train_word2vec_model(WORD2VEC_VECTOR_SIZE)\n",
    "SentimentAnalysis.save_word2vec(word2vec_model, 'word2vec.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c664187",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['or', 'work', 'never', 'water', 'while', 'wearing', 'working', 'worn', 'under', 'etc']\n",
      "\n",
      "Cluster 1\n",
      "['i', 'was', 'had', 'me', 'bought', 'am', 'amazon', 'buy', 'he', 'first']\n",
      "\n",
      "Cluster 2\n",
      "['of', 'that', 'have', 'as', 'one', 'watches', 'than', 'they', 'other', 've']\n",
      "\n",
      "Cluster 3\n",
      "['you', 'time', 'if', 'use', 'day', 'battery', 'do', 'your', 'date', 'set']\n",
      "\n",
      "Cluster 4\n",
      "['it', 'a', 'and', 'watch', 'is', 'this', 'but', 'not', 's', 'very']\n",
      "\n",
      "Cluster 5\n",
      "['years', 'year', 'months', 'days', 'times', 'month', 'week', 'seconds', 'minutes', 'weeks']\n",
      "\n",
      "Cluster 6\n",
      "['to', 'in', 'my', 'with', 't', 'so', 'at', 'has', 'can', 'just']\n",
      "\n",
      "Cluster 7\n",
      "['are', 'which', 'face', 'light', 'read', 'see', 'hand', 'case', 'dial', 'hands']\n",
      "\n",
      "Cluster 8\n",
      "['for', 'about', 'after', 'been', 'now', 'two', 'over', '2', '5', 'few']\n",
      "\n",
      "Cluster 9\n",
      "['the', 'on', 'be', 'band', 'out', 'up', 'get', 'off', 'little', 'back']\n"
     ]
    }
   ],
   "source": [
    "SaHandler.clusterize(word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42528eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SaHandler.divideTrainTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cb31c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SaHandler.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d05264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SaHandler.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment-analysis",
   "language": "python",
   "name": "sentiment-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
