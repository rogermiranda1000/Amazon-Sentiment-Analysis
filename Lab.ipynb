{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b179ce6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import re\n",
    "import json\n",
    "import datetime\n",
    "from gensim.models import word2vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Activation, TimeDistributed\n",
    "\n",
    "import multiprocessing # number of threads\n",
    "\n",
    "import matplotlib.pyplot as plt # plot\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from typing import Dict,Any,List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c523f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysis():\n",
    "    def __init__(self, json_file, hidden_layer = (300,50), train_test_split: float = 0.7, random_state: int = None):\n",
    "        self._reviews=json.load(gzip.open(json_file,'rb'))\n",
    "        \n",
    "        # split in train-test\n",
    "        self._test_reviews=self._reviews[int(len(self._reviews)*train_test_split):]\n",
    "        self._reviews=self._reviews[:-len(self._test_reviews)]\n",
    "        \n",
    "        # save variables for the rest of the functions\n",
    "        self._hidden_layer = hidden_layer\n",
    "        self._random_state = random_state\n",
    "        self._train_test_split = train_test_split\n",
    "    \n",
    "    @property\n",
    "    def reviews(self) -> List[Dict[str,Any]]:\n",
    "        return self._reviews\n",
    "    \n",
    "    @property\n",
    "    def largest_review_size(self) -> int:\n",
    "        # @ref https://stackoverflow.com/a/1582670/9178470\n",
    "        return len(max(self.review_sentences, key=len))\n",
    "        \n",
    "    @property\n",
    "    def review_sentences(self) -> List[str]:\n",
    "        return [x['text'] for x in self._reviews]\n",
    "    \n",
    "    def makeBoW(self):\n",
    "        count_vectorizer=CountVectorizer()\n",
    "        counts=count_vectorizer.fit_transform([\" \".join(x['text']) for x in self._reviews])\n",
    "        transformer = TfidfTransformer()\n",
    "        self.BoW=transformer.fit_transform(counts)\n",
    "        #Posem els scores en un array\n",
    "        self.scores=np.array([x['score'] for x in self._reviews])\n",
    "\n",
    "    def learn(self):\n",
    "        #To be fit by the student\n",
    "        learner=MLPClassifier(hidden_layer_sizes=self._hidden_layer, solver='lbfgs', alpha=1e-5, random_state=self._random_state)\n",
    "        self.model=learner.fit(self.BoW_train,self.scores_train)\n",
    "        pass\n",
    "    \n",
    "    def evaluate(self):\n",
    "        #Returns RMSE\n",
    "        print(np.sqrt(sum(np.power(self.scores_test-self.model.predict(self.BoW_test),2))))\n",
    "        print(confusion_matrix(self.scores_test,np.round(self.model.predict(self.BoW_test))))\n",
    "        print(classification_report(self.scores_test,np.round(self.model.predict(self.BoW_test))))\n",
    "\n",
    "    def train_word2vec_model(self, size: int, window: int = 5, min_count: int = 5, workers: int = -1) -> word2vec.Word2Vec:\n",
    "        \"\"\"\n",
    "        Get a word2vec model according to the input data.\n",
    "        @ref https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
    "        \n",
    "        @param size:      Dimensionality of the word vectors\n",
    "        @param window:    Maximum distance between the current and predicted word\n",
    "        @param min_count: Ignores all words with total frequency lower than this\n",
    "        @param workers:   Use these many worker threads to train the model; -1 to use all\n",
    "        @return Word2Vec model\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        sentences=self.review_sentences\n",
    "        \n",
    "        if workers == -1:\n",
    "            workers = SentimentAnalysis.get_num_max_workers()\n",
    "            print(f\"[v] Using {workers} workers for the Word2Vec operation...\")\n",
    "            \n",
    "        return word2vec.Word2Vec(sentences, vector_size=size, window=window,\n",
    "                                       min_count=min_count, workers=workers)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_num_max_workers() -> int:\n",
    "        return multiprocessing.cpu_count()\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_word2vec(model: word2vec.Word2Vec, file_name: str):\n",
    "        model.save(file_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_word2vec(file_name: str) -> word2vec.Word2Vec:\n",
    "        return word2vec.Word2Vec.load(file_name)\n",
    "\n",
    "# global variables\n",
    "WORD2VEC_VECTOR_SIZE = 300\n",
    "HIDDEN_DIM = 52 # must be a power of 2\n",
    "\n",
    "# create an inscance of the class\n",
    "SaHandler=SentimentAnalysis('data/Watches_withstopwords.json.gz', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18eca26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0: having owned two previous g shocks in my life including the first series in 1984 i ve long appreciated their quality the main reason i stopped wearing them was simply because the resin straps would break and having worn them for 4 years i grew weary of them however having burned through many fashion watches in the last 10 years i ve been disappointed to spend only to get 1 or 2 years worth of usage so i m back to g shock i think this model g1710d 7av represents a nice blend between good looks and practical durability which most guys really want the face is smaller on my wrist than i expected from a g shock but i think it s still classy for the office the side buttons are hidden in gray plastic which you can t see in picture but the rest of the watch is metal you might find a classier analog face g shock in the 200 300 but this one is a better value the only reason i didn t give it 5 stars is because the led isn t backlight it s an amber light that comes around the inside bevel so the led helps you read the dial hands but not the lcd screens nonetheless it s still a cool light color ps the bottom lcd screen isn t blue as pictured but it s still a different color than the top lcd which adds further distinction to the watch\n"
     ]
    }
   ],
   "source": [
    "print(f\"{SaHandler.reviews[0]['score']}: {' '.join(SaHandler.reviews[0]['text'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb0cc499",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4187\n"
     ]
    }
   ],
   "source": [
    "max_input_size = SaHandler.largest_review_size\n",
    "print(max_input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc707f4",
   "metadata": {},
   "source": [
    "Here we'll train a Word2Vec model using the train data.\n",
    "Word2Vec will learn word associations from a large corpus of text. It will be able to detect synonymous words,[ref](https://en.wikipedia.org/w/index.php?title=Word2vec&oldid=1143734439) as words are represented by vectors that states the context of that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "674e3b7f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v] Using 32 workers for the Word2Vec operation...\n"
     ]
    }
   ],
   "source": [
    "word2vec_model = SaHandler.train_word2vec_model(WORD2VEC_VECTOR_SIZE)\n",
    "SentimentAnalysis.save_word2vec(word2vec_model, 'word2vec.bin')\n",
    "word_vectors = word2vec_model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b798ff",
   "metadata": {},
   "source": [
    "What if we visualize the generated word2vec model?\n",
    "In order to do it we'll need some kind of clustering algorithm; we'll use k-Means. I've also tried DBSCAN and OPTICS, as they are other methods found on [sk-learn](https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods), but it seems like there's no relevant separation to use a density-based algorithm, as they both report one single group.\n",
    "\n",
    "Some information needed to understand what we'll do next:\n",
    "- `Word2Vec#wv.vectors` will return a matrix of `<number of input words> rows x <vector size> columns`, representing (for each input word) its vector\n",
    "- `Word2Vec#wv.index_to_key` will return a list with all the input words. This will be useful in order to relate a vector to an actual word\n",
    "Also, the section [\"What can I do with word vectors?\", on gensim wiki](https://radimrehurek.com/gensim/models/keyedvectors.html#what-can-i-do-with-word-vectors) is very interesting to see the word extrapolation, but it won't be discussed in this Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fa41da8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: ['can', 'don', 'doesn', 'didn', 'won', 'isn', 'haven', 'couldn', 'wasn', 'wouldn']\n",
      "Cluster 1: ['the', 'that', 'but', 'not', 'band', 'than', 'which', 'face', 'other', 'wrist']\n",
      "Cluster 2: ['i', 'was', 'one', 'had', 'watches', 'me', 'price', 'they', 'bought', 'years']\n",
      "Cluster 3: ['time', 'only', 'day', 'light', 'there', 'date', 'second', 'set', 'how', 'hand']\n",
      "Cluster 4: ['water']\n",
      "Cluster 5: ['a', 'of', 'my', 'with', 'as', 'so', 'at', 'has', 'are', 'would']\n",
      "Cluster 6: ['to', 'in', 'on', 'or', 'just', 'when', 'out', 'up', 'still', 'off']\n",
      "Cluster 7: ['it', 'and', 'watch', 'is', 'this', 's', 'very', 'great', 'like', 'good']\n",
      "Cluster 8: ['have', 'you', 't', 'be', 'if', 'get', 'wear', 'use', 'do', 'buy']\n",
      "Cluster 9: ['for', 'about', 'after', 'two', 'over', '2', '5', 'few', '3', '1']\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 10\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters, n_init='auto' )\n",
    "# `fit_predict` will force each of the `word_vectors.vectors` vectors into one of the 10 clusters\n",
    "idx = kmeans_clustering.fit_predict( word_vectors.vectors )\n",
    "\n",
    "# Create a Word / Index dictionary, mapping each vocabulary word to a cluster number\n",
    "word_centroid_map = dict(list(zip( word_vectors.index_to_key, idx )))\n",
    "\n",
    "# For each cluster\n",
    "for cluster in range(num_clusters):\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    for key, value in zip(list(word_centroid_map.keys()),list(word_centroid_map.values())):\n",
    "        if (value == cluster):\n",
    "            words.append(key)\n",
    "    \n",
    "    print(f\"Cluster {cluster}: {words[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f443f09",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 300)\n",
      "(2, 5)\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7268 - accuracy: 0.0000e+00\n",
      "Layer 0: (None, 2, 300), (None, 52)\n",
      "Layer 1: (None, 52), (None, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 02:54:20.884995: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-03-24 02:54:20.953456: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-03-24 02:54:20.953735: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-03-24 02:54:20.954013: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    }
   ],
   "source": [
    "# TODO Average of Word2Vec vectors with TF-IDF : this is one of the best approach which I will recommend. Just take the word vectors and multiply it with their TF-IDF scores. Just take the average and it will represent your sentence vector.\n",
    "# @ref https://stackoverflow.com/a/31738627/9178470\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, 2, 300))\n",
    "\n",
    "model = Sequential()\n",
    "# Assuming that your input size (X.shape) is n X t X f where\n",
    "# n:Batch size\n",
    "# t: sequence length/time-steps/no:of unrollings)\n",
    "# f: Nºof feature per time-step\n",
    "# Note: input_shape=(t,f)\n",
    "# @ref https://stackoverflow.com/a/62994263/9178470\n",
    "vector_processor = Bidirectional(LSTM(HIDDEN_DIM // 2, return_sequences=False),\n",
    "                        input_shape=(inputs.shape[-2], inputs.shape[-1]),\n",
    "                        merge_mode='concat') # concat will return n X t X <HIDDEN_DIM/2>*2 # TODO why *2 and not *<nº of LSTM>?\n",
    "model.add(vector_processor)\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "#model.add(Activation('softmax'))\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "X = np.stack(( np.stack((word_vectors.vectors[0], word_vectors.vectors[1])),\n",
    "             np.stack((word_vectors.vectors[3], word_vectors.vectors[4])) )) # example of phrases (<num inputs>x<max words of review>x<vector size>)\n",
    "y = np.stack(( np.array([0,0,0,0,1]), np.array([0,1,0,0,0]) )) # score 5 and 2\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "model.fit(X, y, epochs=1, batch_size=32)\n",
    "#model(X)\n",
    "\n",
    "for i,layer in enumerate(model.layers):\n",
    "    print(f\"Layer {i}: {layer.input_shape}, {layer.output_shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c0e48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment-analysis",
   "language": "python",
   "name": "sentiment-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
