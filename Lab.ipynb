{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b179ce6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 12:57:20.590854: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import re\n",
    "import json\n",
    "import datetime\n",
    "from gensim.models import word2vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Activation, TimeDistributed\n",
    "\n",
    "import multiprocessing # number of threads\n",
    "\n",
    "import matplotlib.pyplot as plt # plot\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from typing import Dict,Any,List,Generator,Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c523f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysis():\n",
    "    def __init__(self, json_file, hidden_layer = (300,50), train_test_split: float = 0.7, random_state: int = None):\n",
    "        self._reviews=json.load(gzip.open(json_file,'rb'))\n",
    "        \n",
    "        # split in train-test\n",
    "        self._test_reviews=self._reviews[int(len(self._reviews)*train_test_split):]\n",
    "        self._reviews=self._reviews[:-len(self._test_reviews)]\n",
    "        \n",
    "        # save variables for the rest of the functions\n",
    "        self._hidden_layer = hidden_layer\n",
    "        self._random_state = random_state\n",
    "        self._train_test_split = train_test_split\n",
    "    \n",
    "    @property\n",
    "    def reviews(self) -> List[Dict[str,Any]]:\n",
    "        return self._reviews\n",
    "    \n",
    "    def padded_reviews(self, vectorizer: word2vec.Word2Vec, batch_size: int = -1, iterate_forever: bool = False) -> Generator[None,Tuple[np.ndarray,np.ndarray],None]:\n",
    "        \"\"\"\n",
    "        Returns all the train reviews in a matrix, padding the sentences so all have the same size\n",
    "        @param vectorizer      Word2Vec object to convert from word to vector\n",
    "        @param batch_size      Number of reviews each returned ndarray has. Use -1 if use all the reviews.\n",
    "        @param iterate_forever If True, it will iterate indefinitely\n",
    "        @return Generator of ndarray of size (<batch_size>,<max words in review>,<Word2Vec return vector lenght>),\n",
    "                and its expected value (<batch_size>,5)\n",
    "        \"\"\"\n",
    "        vector_size = vectorizer.wv[0].shape\n",
    "        reviews = [x['text'] for x in self._reviews]\n",
    "        y_reviews = [int(x['score']) for x in self._reviews]\n",
    "        largest_review_size = self.largest_review_size\n",
    "        if batch_size == -1: batch_size = len(reviews)\n",
    "        \n",
    "        offset = 0\n",
    "        while True:\n",
    "            reviews_slice = reviews[offset*batch_size:(offset+1)*batch_size]\n",
    "            y_reviews_slice = y_reviews[offset*batch_size:(offset+1)*batch_size]\n",
    "            r = np.zeros(shape=( len(reviews_slice),largest_review_size,vector_size[-1] ))\n",
    "            y = np.zeros(shape=( len(reviews_slice),5 ))\n",
    "            \n",
    "            for review_index,review in enumerate(reviews_slice):\n",
    "                review_offset = largest_review_size-len(review) # will help with the padding\n",
    "                for word_index,word in enumerate(review):\n",
    "                    if word in vectorizer.wv: # if not, leave it as 0\n",
    "                        r[review_index,review_offset+word_index,:] = vectorizer.wv[word]\n",
    "                y[review_index,y_reviews_slice[review_index]-1] = 1 # the index is from 1 to 5\n",
    "            \n",
    "            yield (r,y)\n",
    "            \n",
    "            offset += 1\n",
    "            if offset >= len(reviews):\n",
    "                offset = 0\n",
    "                \n",
    "                if not iterate_forever:\n",
    "                    return\n",
    "    \n",
    "    @property\n",
    "    def largest_review_size(self) -> int:\n",
    "        # @ref https://stackoverflow.com/a/1582670/9178470\n",
    "        return len(max(self.review_sentences, key=len))\n",
    "        \n",
    "    @property\n",
    "    def review_sentences(self) -> List[str]:\n",
    "        return [x['text'] for x in self._reviews]\n",
    "    \n",
    "    @property\n",
    "    def num_reviews(self) -> int:\n",
    "        return len(self._reviews)\n",
    "\n",
    "    def train_word2vec_model(self, size: int, window: int = 5, min_count: int = 5, workers: int = -1) -> word2vec.Word2Vec:\n",
    "        \"\"\"\n",
    "        Get a word2vec model according to the input data.\n",
    "        @ref https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
    "        \n",
    "        @param size:      Dimensionality of the word vectors\n",
    "        @param window:    Maximum distance between the current and predicted word\n",
    "        @param min_count: Ignores all words with total frequency lower than this\n",
    "        @param workers:   Use these many worker threads to train the model; -1 to use all\n",
    "        @return Word2Vec model\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        sentences=self.review_sentences\n",
    "        \n",
    "        if workers == -1:\n",
    "            workers = SentimentAnalysis.get_num_max_workers()\n",
    "            print(f\"[v] Using {workers} workers for the Word2Vec operation...\")\n",
    "            \n",
    "        return word2vec.Word2Vec(sentences, vector_size=size, window=window,\n",
    "                                       min_count=min_count, workers=workers)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_num_max_workers() -> int:\n",
    "        return multiprocessing.cpu_count()\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_word2vec(model: word2vec.Word2Vec, file_name: str):\n",
    "        model.save(file_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_word2vec(file_name: str) -> word2vec.Word2Vec:\n",
    "        return word2vec.Word2Vec.load(file_name)\n",
    "\n",
    "# global variables\n",
    "WORD2VEC_VECTOR_SIZE = 300\n",
    "INPUT_HIDDEN_DIM = 52 # must be a power of 2\n",
    "DENSE_HIDDEN_DIM = (200,50)\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 3\n",
    "\n",
    "# create an inscance of the class\n",
    "SaHandler=SentimentAnalysis('data/Watches_withstopwords.json.gz', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18eca26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0: having owned two previous g shocks in my life including the first series in 1984 i ve long appreciated their quality the main reason i stopped wearing them was simply because the resin straps would break and having worn them for 4 years i grew weary of them however having burned through many fashion watches in the last 10 years i ve been disappointed to spend only to get 1 or 2 years worth of usage so i m back to g shock i think this model g1710d 7av represents a nice blend between good looks and practical durability which most guys really want the face is smaller on my wrist than i expected from a g shock but i think it s still classy for the office the side buttons are hidden in gray plastic which you can t see in picture but the rest of the watch is metal you might find a classier analog face g shock in the 200 300 but this one is a better value the only reason i didn t give it 5 stars is because the led isn t backlight it s an amber light that comes around the inside bevel so the led helps you read the dial hands but not the lcd screens nonetheless it s still a cool light color ps the bottom lcd screen isn t blue as pictured but it s still a different color than the top lcd which adds further distinction to the watch\n"
     ]
    }
   ],
   "source": [
    "print(f\"{SaHandler.reviews[0]['score']}: {' '.join(SaHandler.reviews[0]['text'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb0cc499",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4187\n",
      "85.73769566762105\n"
     ]
    }
   ],
   "source": [
    "max_input_size = SaHandler.largest_review_size\n",
    "print(max_input_size)\n",
    "\n",
    "avg_lenght = 0\n",
    "for review in SaHandler.review_sentences:\n",
    "    avg_lenght += len(review)\n",
    "avg_lenght = avg_lenght / SaHandler.num_reviews\n",
    "print(avg_lenght)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc707f4",
   "metadata": {},
   "source": [
    "Here we'll train a Word2Vec model using the train data.\n",
    "Word2Vec will learn word associations from a large corpus of text. It will be able to detect synonymous words,[ref](https://en.wikipedia.org/w/index.php?title=Word2vec&oldid=1143734439) as words are represented by vectors that states the context of that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "674e3b7f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v] Using 32 workers for the Word2Vec operation...\n"
     ]
    }
   ],
   "source": [
    "word2vec_model = SaHandler.train_word2vec_model(WORD2VEC_VECTOR_SIZE)\n",
    "SentimentAnalysis.save_word2vec(word2vec_model, 'word2vec.bin')\n",
    "word_vectors = word2vec_model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b798ff",
   "metadata": {},
   "source": [
    "What if we visualize the generated word2vec model?\n",
    "In order to do it we'll need some kind of clustering algorithm; we'll use k-Means. I've also tried DBSCAN and OPTICS, as they are other methods found on [sk-learn](https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods), but it seems like there's no relevant separation to use a density-based algorithm, as they both report one single group.\n",
    "\n",
    "Some information needed to understand what we'll do next:\n",
    "- `Word2Vec#wv.vectors` will return a matrix of `<number of input words> rows x <vector size> columns`, representing (for each input word) its vector\n",
    "- `Word2Vec#wv.index_to_key` will return a list with all the input words. This will be useful in order to relate a vector to an actual word\n",
    "\n",
    "Also, the section [\"What can I do with word vectors?\", on gensim wiki](https://radimrehurek.com/gensim/models/keyedvectors.html#what-can-i-do-with-word-vectors) is very interesting to see the word extrapolation, but it won't be discussed in this Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fa41da8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: ['for', 'of', 'in', 'my', 'with', 'so', 'at', 'has', 'are', 'just']\n",
      "Cluster 1: ['have', 'you', 't', 'be', 'if', 'get', 'wear', 'use', 'do', 'buy']\n",
      "Cluster 2: ['more', 'other', 'better', 'most', 'less', 'different', 'picture', 'expected', 'smaller', 'others']\n",
      "Cluster 3: ['years', 'day', 'battery', 'year', 'months', 'days', 'times', 'month', 'week', 'seconds']\n",
      "Cluster 4: ['the', 'on', 'or', 'when', 'out', 'up', 'off', 'even', 'back', 'then']\n",
      "Cluster 5: ['about', 'after', 'now', 'two', 'over', '2', '5', 'few', '3', 'every']\n",
      "Cluster 6: ['time', 'which', 'there', 'date', 'second', 'set', 'hand', 'works', 'hands', 'alarm']\n",
      "Cluster 7: ['i', 'this', 'was', 'one', 'had', 'watches', 'me', 'price', 'they', 'bought']\n",
      "Cluster 8: ['to', 'can', 'would', 'will', 'don', 'does', 'could', 'did', 'doesn', 'should']\n",
      "Cluster 9: ['it', 'a', 'and', 'watch', 'is', 'that', 'but', 'not', 's', 'very']\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 10\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters, n_init='auto' )\n",
    "# `fit_predict` will force each of the `word_vectors.vectors` vectors into one of the 10 clusters\n",
    "idx = kmeans_clustering.fit_predict( word_vectors.vectors )\n",
    "\n",
    "# Create a Word / Index dictionary, mapping each vocabulary word to a cluster number\n",
    "word_centroid_map = dict(list(zip( word_vectors.index_to_key, idx )))\n",
    "\n",
    "# For each cluster\n",
    "for cluster in range(num_clusters):\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    for key, value in zip(list(word_centroid_map.keys()),list(word_centroid_map.values())):\n",
    "        if (value == cluster):\n",
    "            words.append(key)\n",
    "    \n",
    "    print(f\"Cluster {cluster}: {words[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f443f09",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 12:57:29.852425: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-24 12:57:29.880891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-24 12:57:29.881274: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-24 12:57:29.882085: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-24 12:57:29.885338: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-24 12:57:29.885643: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-24 12:57:29.885933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-24 12:57:30.515452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-24 12:57:30.515673: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-24 12:57:30.515683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-03-24 12:57:30.515880: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-24 12:57:30.515930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5389 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:0b:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 12:57:34.630540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n",
      "2023-03-24 12:57:34.802815: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-03-24 12:57:34.830587: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x56137b10af10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-03-24 12:57:34.830626: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3070, Compute Capability 8.6\n",
      "2023-03-24 12:57:34.843125: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-03-24 12:57:34.844042: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:234] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.6\n",
      "2023-03-24 12:57:34.844069: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:237] Used ptxas at ptxas\n",
      "2023-03-24 12:57:34.844114: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:281] Couldn't read CUDA driver version.\n",
      "2023-03-24 12:57:34.844288: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2023-03-24 12:57:34.852593: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-03-24 12:57:34.863860: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-03-24 12:57:35.225028: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-03-24 12:57:35.236063: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-03-24 12:57:35.272137: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-03-24 12:57:35.284034: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-03-24 12:57:35.295369: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1/374 [..............................] - ETA: 24:29 - loss: 0.6963 - accuracy: 0.2891"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 12:57:35.481607: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-03-24 12:57:35.482375: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-03-24 12:57:35.482409: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374/374 [==============================] - 138s 359ms/step - loss: 0.4549 - accuracy: 0.5594\n",
      "Epoch 2/3\n",
      "374/374 [==============================] - 136s 363ms/step - loss: 0.3906 - accuracy: 0.5739\n",
      "Epoch 3/3\n",
      "374/374 [==============================] - 135s 360ms/step - loss: 0.3875 - accuracy: 0.5739\n",
      "(128, 4187, 300)\n",
      "(128, 5)\n",
      "Layer 0: (None, 4187, 300), (None, 52)\n",
      "Layer 1: (None, 52), (None, 400)\n",
      "Layer 2: (None, 400), (None, 100)\n",
      "Layer 3: (None, 100), (None, 50)\n",
      "Layer 4: (None, 50), (None, 5)\n"
     ]
    }
   ],
   "source": [
    "# @ref https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, sa: SentimentAnalysis, word2vec_model: word2vec.Word2Vec, batch_size: int = 32):\n",
    "        self._vector_size = word2vec_model.wv[0].shape\n",
    "        self._reviews = [x['text'] for x in sa._reviews]\n",
    "        self._y_reviews = [int(x['score']) for x in sa._reviews]\n",
    "        self._largest_review_size = sa.largest_review_size\n",
    "        self._vectorizer = word2vec_model\n",
    "        \n",
    "        self._generator = sa.padded_reviews(word2vec_model, batch_size)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_batches_per_epoch = -(sa.num_reviews // -batch_size) # ceil; @ref https://stackoverflow.com/a/17511341/9178470\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self._num_batches_per_epoch\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[np.ndarray,np.ndarray]:\n",
    "        reviews_slice = self._reviews[idx*self._batch_size:(idx+1)*self._batch_size]\n",
    "        y_reviews_slice = self._y_reviews[idx*self._batch_size:(idx+1)*self._batch_size]\n",
    "        r = np.zeros(shape=( len(reviews_slice),self._largest_review_size,self._vector_size[-1] ))\n",
    "        y = np.zeros(shape=( len(reviews_slice),5 ))\n",
    "\n",
    "        for review_index,review in enumerate(reviews_slice):\n",
    "            review_offset = self._largest_review_size-len(review) # will help with the padding\n",
    "            for word_index,word in enumerate(review):\n",
    "                if word in self._vectorizer.wv: # if not, leave it as 0\n",
    "                    r[review_index,review_offset+word_index,:] = self._vectorizer.wv[word]\n",
    "            y[review_index,y_reviews_slice[review_index]-1] = 1 # the index is from 1 to 5\n",
    "\n",
    "        return (r,y)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, SaHandler.largest_review_size, WORD2VEC_VECTOR_SIZE))\n",
    "\n",
    "model = Sequential()\n",
    "# Assuming that your input size (X.shape) is n X t X f where\n",
    "# n:Batch size\n",
    "# t: sequence length/time-steps/no:of unrollings)\n",
    "# f: Nºof feature per time-step\n",
    "# Note: input_shape=(t,f)\n",
    "# @ref https://stackoverflow.com/a/62994263/9178470\n",
    "vector_processor = Bidirectional(LSTM(INPUT_HIDDEN_DIM // 2, return_sequences=False),\n",
    "                        input_shape=(inputs.shape[-2], inputs.shape[-1]),\n",
    "                        merge_mode='concat') # concat will return n X t X <HIDDEN_DIM/2>*2 # TODO why *2 and not *<nº of LSTM>?\n",
    "model.add(vector_processor)\n",
    "for hidden_dense in DENSE_HIDDEN_DIM:\n",
    "    model.add(Dense(hidden_dense))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "#model.add(Activation('softmax'))\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "training_generator = DataGenerator(SaHandler, word2vec_model, BATCH_SIZE)\n",
    "model.fit(training_generator, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(training_generator[0][0].shape)\n",
    "print(training_generator[0][1].shape)\n",
    "for i,layer in enumerate(model.layers):\n",
    "    print(f\"Layer {i}: {layer.input_shape}, {layer.output_shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8183507",
   "metadata": {},
   "source": [
    "As it seems like the LSTM didn't suceed (I've tried multiple combinations of size, and none was better than 0.6) we'll try [neel aproach, on How to get vector for a sentence from the word2vec of tokens in sentence](https://stackoverflow.com/a/31738627/9178470), by making the average of Word2Vec vectors with TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2d8c0e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '0000' ... 'zwitzerland' 'zxc' 'zzzffth']\n",
      "(47849, 42034)\n",
      "(300,)\n",
      "18493\n",
      "(300,)\n",
      "26877\n",
      "(300,)\n",
      "38692\n",
      "(300,)\n",
      "28855\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "33381\n",
      "(300,)\n",
      "19699\n",
      "(300,)\n",
      "24999\n",
      "(300,)\n",
      "22137\n",
      "(300,)\n",
      "19768\n",
      "(300,)\n",
      "37136\n",
      "(300,)\n",
      "15866\n",
      "(300,)\n",
      "33022\n",
      "(300,)\n",
      "19699\n",
      "(300,)\n",
      "603\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "39783\n",
      "(300,)\n",
      "22540\n",
      "(300,)\n",
      "4285\n",
      "(300,)\n",
      "37190\n",
      "(300,)\n",
      "29666\n",
      "(300,)\n",
      "37136\n",
      "(300,)\n",
      "23092\n",
      "(300,)\n",
      "30209\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "35404\n",
      "(300,)\n",
      "40785\n",
      "(300,)\n",
      "37200\n",
      "(300,)\n",
      "40449\n",
      "(300,)\n",
      "33716\n",
      "(300,)\n",
      "5798\n",
      "(300,)\n",
      "37136\n",
      "(300,)\n",
      "31232\n",
      "(300,)\n",
      "35486\n",
      "(300,)\n",
      "41534\n",
      "(300,)\n",
      "6844\n",
      "(300,)\n",
      "3906\n",
      "(300,)\n",
      "18493\n",
      "(300,)\n",
      "41496\n",
      "(300,)\n",
      "37200\n",
      "(300,)\n",
      "16240\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "41825\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "17824\n",
      "(300,)\n",
      "40799\n",
      "(300,)\n",
      "26067\n",
      "(300,)\n",
      "37200\n",
      "(300,)\n",
      "19104\n",
      "(300,)\n",
      "18493\n",
      "(300,)\n",
      "7235\n",
      "(300,)\n",
      "37475\n",
      "(300,)\n",
      "23307\n",
      "(300,)\n",
      "15386\n",
      "(300,)\n",
      "40532\n",
      "(300,)\n",
      "19699\n",
      "(300,)\n",
      "37136\n",
      "(300,)\n",
      "21792\n",
      "(300,)\n",
      "121\n",
      "(300,)\n",
      "41825\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "39783\n",
      "(300,)\n",
      "5843\n",
      "(300,)\n",
      "12328\n",
      "(300,)\n",
      "37851\n",
      "(300,)\n",
      "34797\n",
      "(300,)\n",
      "26264\n",
      "(300,)\n",
      "37851\n",
      "(300,)\n",
      "17186\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "26414\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "41825\n",
      "(300,)\n",
      "41516\n",
      "(300,)\n",
      "26067\n",
      "(300,)\n",
      "39549\n",
      "(300,)\n",
      "34401\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "5269\n",
      "(300,)\n",
      "37851\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "33371\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "37352\n",
      "(300,)\n",
      "37380\n",
      "(300,)\n",
      "24391\n",
      "(300,)\n",
      "2063\n",
      "(300,)\n",
      "31097\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "25397\n",
      "(300,)\n",
      "6343\n",
      "(300,)\n",
      "6085\n",
      "(300,)\n",
      "17503\n",
      "(300,)\n",
      "22599\n",
      "(300,)\n",
      "3906\n",
      "(300,)\n",
      "28565\n",
      "(300,)\n",
      "13213\n",
      "(300,)\n",
      "41023\n",
      "(300,)\n",
      "24643\n",
      "(300,)\n",
      "18057\n",
      "(300,)\n",
      "30180\n",
      "(300,)\n",
      "40363\n",
      "(300,)\n",
      "37136\n",
      "(300,)\n",
      "15171\n",
      "(300,)\n",
      "20698\n",
      "(300,)\n",
      "34178\n",
      "(300,)\n",
      "26223\n",
      "(300,)\n",
      "24999\n",
      "(300,)\n",
      "41600\n",
      "(300,)\n",
      "37089\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "14887\n",
      "(300,)\n",
      "16608\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "33371\n",
      "(300,)\n",
      "7281\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "37352\n",
      "(300,)\n",
      "20773\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "35327\n",
      "(300,)\n",
      "8747\n",
      "(300,)\n",
      "16240\n",
      "(300,)\n",
      "37136\n",
      "(300,)\n",
      "26100\n",
      "(300,)\n",
      "37136\n",
      "(300,)\n",
      "33551\n",
      "(300,)\n",
      "7323\n",
      "(300,)\n",
      "4416\n",
      "(300,)\n",
      "18763\n",
      "(300,)\n",
      "19699\n",
      "(300,)\n",
      "17760\n",
      "(300,)\n",
      "28070\n",
      "(300,)\n",
      "41023\n",
      "(300,)\n",
      "41893\n",
      "(300,)\n",
      "7575\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "32794\n",
      "(300,)\n",
      "19699\n",
      "(300,)\n",
      "27856\n",
      "(300,)\n",
      "7281\n",
      "(300,)\n",
      "37136\n",
      "(300,)\n",
      "31350\n",
      "(300,)\n",
      "26067\n",
      "(300,)\n",
      "37136\n",
      "(300,)\n",
      "40486\n",
      "(300,)\n",
      "20698\n",
      "(300,)\n",
      "23876\n",
      "(300,)\n",
      "41893\n",
      "(300,)\n",
      "24032\n",
      "(300,)\n",
      "15777\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "8731\n",
      "(300,)\n",
      "3872\n",
      "(300,)\n",
      "15171\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "33371\n",
      "(300,)\n",
      "19699\n",
      "(300,)\n",
      "37136\n",
      "(300,)\n",
      "694\n",
      "(300,)\n",
      "1168\n",
      "(300,)\n",
      "7281\n",
      "(300,)\n",
      "37380\n",
      "(300,)\n",
      "26239\n",
      "(300,)\n",
      "20698\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "6071\n",
      "(300,)\n",
      "39715\n",
      "(300,)\n",
      "37136\n",
      "(300,)\n",
      "26264\n",
      "(300,)\n",
      "30209\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "12076\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "17292\n",
      "(300,)\n",
      "20773\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "35142\n",
      "(300,)\n",
      "20698\n",
      "(300,)\n",
      "5798\n",
      "(300,)\n",
      "37136\n",
      "(300,)\n",
      "21958\n",
      "(300,)\n",
      "20736\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "5295\n",
      "(300,)\n",
      "20773\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "3864\n",
      "(300,)\n",
      "3766\n",
      "(300,)\n",
      "22173\n",
      "(300,)\n",
      "37113\n",
      "(300,)\n",
      "9162\n",
      "(300,)\n",
      "4492\n",
      "(300,)\n",
      "37136\n",
      "(300,)\n",
      "20214\n",
      "(300,)\n",
      "6096\n",
      "(300,)\n",
      "34401\n",
      "(300,)\n",
      "37136\n",
      "(300,)\n",
      "21958\n",
      "(300,)\n",
      "18670\n",
      "(300,)\n",
      "41893\n",
      "(300,)\n",
      "30095\n",
      "(300,)\n",
      "37136\n",
      "(300,)\n",
      "12012\n",
      "(300,)\n",
      "18286\n",
      "(300,)\n",
      "7281\n",
      "(300,)\n",
      "25655\n",
      "(300,)\n",
      "37136\n",
      "(300,)\n",
      "21883\n",
      "(300,)\n",
      "32619\n",
      "(300,)\n",
      "25582\n",
      "(300,)\n",
      "20773\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "35327\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "10050\n",
      "(300,)\n",
      "22173\n",
      "(300,)\n",
      "9092\n",
      "(300,)\n",
      "29347\n",
      "(300,)\n",
      "37136\n",
      "(300,)\n",
      "6660\n",
      "(300,)\n",
      "21883\n",
      "(300,)\n",
      "32615\n",
      "(300,)\n",
      "20736\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "6427\n",
      "(300,)\n",
      "4566\n",
      "(300,)\n",
      "27858\n",
      "(300,)\n",
      "7281\n",
      "(300,)\n",
      "20773\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "35327\n",
      "(300,)\n",
      "None\n",
      "(300,)\n",
      "12116\n",
      "(300,)\n",
      "9092\n",
      "(300,)\n",
      "37089\n",
      "(300,)\n",
      "37136\n",
      "(300,)\n",
      "37993\n",
      "(300,)\n",
      "21883\n",
      "(300,)\n",
      "41023\n",
      "(300,)\n",
      "2940\n",
      "(300,)\n",
      "16830\n",
      "(300,)\n",
      "12623\n",
      "(300,)\n",
      "37851\n",
      "(300,)\n",
      "37136\n",
      "(300,)\n",
      "40486\n",
      "  (0, 41893)\t0.08593629626414472\n",
      "  (0, 41825)\t0.11502015265570707\n",
      "  (0, 41600)\t0.03970994572684249\n",
      "  (0, 41534)\t0.03437766437221496\n",
      "  (0, 41516)\t0.05494799836661317\n",
      "  (0, 41496)\t0.054858590036912076\n",
      "  (0, 41023)\t0.11985255028438252\n",
      "  (0, 40799)\t0.11675034181562381\n",
      "  (0, 40785)\t0.04885490699836834\n",
      "  (0, 40532)\t0.03322416926773837\n",
      "  (0, 40486)\t0.030548866509352998\n",
      "  (0, 40449)\t0.026160367752400934\n",
      "  (0, 40363)\t0.04872186083649438\n",
      "  (0, 39783)\t0.08259423256491653\n",
      "  (0, 39715)\t0.055541067424059946\n",
      "  (0, 39549)\t0.09210879722344122\n",
      "  (0, 38692)\t0.045063034448750705\n",
      "  (0, 37993)\t0.06361607830164367\n",
      "  (0, 37851)\t0.07221711506153189\n",
      "  (0, 37475)\t0.055354583305595055\n",
      "  (0, 37380)\t0.0345863236722287\n",
      "  (0, 37352)\t0.09653745005291114\n",
      "  (0, 37200)\t0.1401815033731899\n",
      "  (0, 37190)\t0.05514268978028273\n",
      "  (0, 37136)\t0.2870481275899318\n",
      "  :\t:\n",
      "  (0, 6844)\t0.06701427276273443\n",
      "  (0, 6660)\t0.06988168435214079\n",
      "  (0, 6427)\t0.058479933256758404\n",
      "  (0, 6343)\t0.09571871222784979\n",
      "  (0, 6096)\t0.11096307131775282\n",
      "  (0, 6085)\t0.06449344826804475\n",
      "  (0, 6071)\t0.04674548619390885\n",
      "  (0, 5843)\t0.041928844000632355\n",
      "  (0, 5798)\t0.08037935764752781\n",
      "  (0, 5295)\t0.07188428697598695\n",
      "  (0, 5269)\t0.04448366216971068\n",
      "  (0, 4566)\t0.028929053608706568\n",
      "  (0, 4492)\t0.04995268183635196\n",
      "  (0, 4416)\t0.03347207329518053\n",
      "  (0, 4285)\t0.09479206130823623\n",
      "  (0, 3906)\t0.03152672271833149\n",
      "  (0, 3872)\t0.06154230695937853\n",
      "  (0, 3864)\t0.03576435706839375\n",
      "  (0, 3766)\t0.11944292504937755\n",
      "  (0, 2940)\t0.08473129654532492\n",
      "  (0, 2063)\t0.1257387938795791\n",
      "  (0, 1168)\t0.07869665429405147\n",
      "  (0, 694)\t0.07356864042275713\n",
      "  (0, 603)\t0.12048088648263558\n",
      "  (0, 121)\t0.0523828511617577\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer=CountVectorizer()\n",
    "counts=count_vectorizer.fit_transform([' '.join(x) for x in SaHandler.review_sentences])\n",
    "print(count_vectorizer.get_feature_names_out())\n",
    "\n",
    "def get_count_vectorizer_index(searching: str) -> int:\n",
    "    r = np.where(count_vectorizer.get_feature_names_out() == searching)\n",
    "    if len(r[0]) == 0:\n",
    "        return None\n",
    "    return r[0][0]\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "BoW=transformer.fit_transform(counts)\n",
    "BoW_train=BoW[:len(SaHandler.reviews),:]\n",
    "\n",
    "print(BoW_train.shape) # we'll have a vector of size <nº train data> x <nº of different words>, with the value representing the weight of that word\n",
    "\n",
    "\n",
    "for BoW_train_index, review in enumerate(SaHandler._reviews):\n",
    "    for word in review['text']:\n",
    "        if word in word2vec_model.wv:\n",
    "            print(word2vec_model.wv[word].shape)\n",
    "            print(get_count_vectorizer_index(word))\n",
    "    print(BoW_train[BoW_train_index])\n",
    "    #review['text'], int(review['score'])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db9dae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment-analysis",
   "language": "python",
   "name": "sentiment-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
