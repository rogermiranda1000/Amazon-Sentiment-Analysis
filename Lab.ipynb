{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b179ce6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word2vec\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer,TfidfTransformer\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#from sklearn.linear_model import SGDRegressor, SGDClassifier\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneural_network\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLPClassifier\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import re\n",
    "import json\n",
    "import datetime\n",
    "from gensim.models import word2vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "#from sklearn.linear_model import SGDRegressor, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from typing import Dict,Any,List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c523f73",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSentimentAnalysis\u001b[39;00m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, json_file, hidden_layer \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m300\u001b[39m,\u001b[38;5;241m50\u001b[39m), random_state: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reviews\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mload(gzip\u001b[38;5;241m.\u001b[39mopen(json_file,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m, in \u001b[0;36mSentimentAnalysis\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_state \u001b[38;5;241m=\u001b[39m random_state\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreviews\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mList\u001b[49m[Dict[\u001b[38;5;28mstr\u001b[39m,Any]]:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reviews\n\u001b[1;32m     12\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlargest_review_size\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# @ref https://stackoverflow.com/a/1582670/9178470\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "class SentimentAnalysis():\n",
    "    def __init__(self, json_file, hidden_layer = (300,50), random_state: int = None):\n",
    "        self._reviews=json.load(gzip.open(json_file,'rb'))\n",
    "        self._hidden_layer = hidden_layer\n",
    "        self._random_state = random_state\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def reviews(self) -> List[Dict[str,Any]]:\n",
    "        return self._reviews\n",
    "    \n",
    "    @property\n",
    "    def largest_review_size(self) -> int:\n",
    "        # @ref https://stackoverflow.com/a/1582670/9178470\n",
    "        return len(max(self.review_sentences, key=len))\n",
    "        \n",
    "    @property\n",
    "    def review_sentences(self) -> np.array:\n",
    "        return np.array([x['text'] for x in self._reviews])\n",
    "    \n",
    "    def makeBoW(self):\n",
    "        count_vectorizer=CountVectorizer()\n",
    "        counts=count_vectorizer.fit_transform([\" \".join(x['text']) for x in self._reviews])\n",
    "        transformer = TfidfTransformer()\n",
    "        self.BoW=transformer.fit_transform(counts)\n",
    "        #Posem els scores en un array\n",
    "        self.scores=np.array([x['score'] for x in self._reviews])\n",
    "\n",
    "    def divideTrainTest(self):\n",
    "        is2013=[x['year']==2013 for x in self._reviews]\n",
    "\n",
    "        self.BoW_train=self.BoW[~np.array(is2013),:]\n",
    "        self.scores_train=self.scores[~np.array(is2013)]\n",
    "        self.BoW_test=self.BoW[np.array(is2013),:]\n",
    "        self.scores_test=self.scores[np.array(is2013)]\n",
    "        pass\n",
    "\n",
    "    def saveTrainTest(self):\n",
    "        #Mireu np.save i (np.load Opcional)\n",
    "        pass\n",
    "    def learn(self):\n",
    "        #To be fit by the student\n",
    "        learner=MLPClassifier(hidden_layer_sizes=self._hidden_layer, solver='lbfgs', alpha=1e-5, random_state=self._random_state)\n",
    "        self.model=learner.fit(self.BoW_train,self.scores_train)\n",
    "        pass\n",
    "    \n",
    "    def evaluate(self):\n",
    "        #Returns RMSE\n",
    "        print(np.sqrt(sum(np.power(self.scores_test-self.model.predict(self.BoW_test),2))))\n",
    "        print(confusion_matrix(self.scores_test,np.round(self.model.predict(self.BoW_test))))\n",
    "        print(classification_report(self.scores_test,np.round(self.model.predict(self.BoW_test))))\n",
    "\n",
    "    def makeWord2Vec(self):\n",
    "        sentences=self.review_sentences\n",
    "        self.model = word2vec.Word2Vec(sentences, size=300, window=5, min_count=5, workers=12)\n",
    "        pass\n",
    "    \n",
    "    def saveWord2Vec(self,model):\n",
    "        self.model.save(model)\n",
    "        pass\n",
    "\n",
    "    def loadWord2Vec(self,model):\n",
    "        self.model=word2vec.Word2Vec.load(model)\n",
    "\n",
    "    def clusterize(self):\n",
    "        # Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "        # average of 30 words per cluster\n",
    "\n",
    "        word_vectors = self.model.wv.vectors\n",
    "        num_clusters = 10\n",
    "\n",
    "        # Initalize a k-means object and use it to extract centroids\n",
    "        kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "        idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "        # Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "        # a cluster number\n",
    "        word_centroid_map = dict(list(zip( self.model.wv.index2word, idx )))\n",
    "\n",
    "        # For the first 10 clusters\n",
    "        for cluster in range(0,10):\n",
    "            #\n",
    "            # Print the cluster number\n",
    "            print(\"\\nCluster %d\" % cluster)\n",
    "            #\n",
    "            # Find all of the words for that cluster number, and print them out\n",
    "            words = []\n",
    "            for i in range(0,len(list(word_centroid_map.values()))):\n",
    "                if( list(word_centroid_map.values())[i] == cluster ):\n",
    "                    words.append(list(word_centroid_map.keys())[i])\n",
    "            print(words[:10])\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28340023",
   "metadata": {},
   "outputs": [],
   "source": [
    "SaHandler=SentimentAnalysis('data/Watches_withstopwords.json.gz', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18eca26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0: owned two previous g shocks life including first series 1984 long appreciated quality main reason stopped wearing simply resin straps would break worn 4 years grew weary however burned many fashion watches last 10 years disappointed spend get 1 2 years worth usage back g shock think model g1710d 7av represents nice blend good looks practical durability guys really face smaller wrist expected g shock think still classy office side buttons hidden gray plastic see picture rest watch metal might find classier analog face g shock 200 300 one better value reason didn give 5 stars led isn backlight amber light comes around inside bevel led helps read dial hands lcd screens nonetheless still cool light color ps bottom lcd screen isn blue pictured still different color top lcd adds distinction watch\n"
     ]
    }
   ],
   "source": [
    "print(f\"{SaHandler.reviews[0]['score']}: {' '.join(SaHandler.reviews[0]['text'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb0cc499",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SaHandler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mSaHandler\u001b[49m\u001b[38;5;241m.\u001b[39mlargest_review_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SaHandler' is not defined"
     ]
    }
   ],
   "source": [
    "print(SaHandler.largest_review_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "674e3b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68356, 50664)\n"
     ]
    }
   ],
   "source": [
    "SaHandler.makeBoW()\n",
    "# matriu de <nº documents> files x <nº paraules> columnes,\n",
    "# indicant (per cada text) com d'important és l'aparició (si apareix) aquella/es paraula/es\n",
    "# Retorna pesos de regularització L2 (la suma quadràtica és 1)\n",
    "print(SaHandler.BoW.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42528eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SaHandler.divideTrainTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cb31c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SaHandler.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d05264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SaHandler.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment-analysis",
   "language": "python",
   "name": "sentiment-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
